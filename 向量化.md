CUDA 中的 “向量化” 技术，这是通过让单个线程处理多个数据元素、充分利用 GPU 的 SIMT 架构和内存带宽来提升并行效率的核心优化手段。
1. CUDA 向量化的核心逻辑
CUDA 的向量化本质是单线程多数据（SIMD）思想在 GPU SIMT 架构上的落地：
传统方式：1 个线程处理 1 个数据元素；
向量化方式：1 个线程处理 2/4/8 个数据元素（如float2/float4、half2/half4）；
核心收益：减少线程总数、降低线程调度开销，同时利用 GPU 的内存合并访问和打包数据类型提升内存吞吐量。
2. CUDA 向量化的实现方式
（1）基础：使用打包数据类型（最常用）
CUDA 提供了原生的打包数据类型（vector types），对应不同数据宽度的向量化处理，核心类型如下：
基础类型	2 元素打包	4 元素打包	8 元素打包	16 元素打包
char	char2	char4	char8	char16
int	int2	int4	int8	int16
float	float2	float4	float8	float16
half	half2	half4	-	-
示例：float4 向量化加法（1 线程处理 4 个 float）
cpp
运行
#include <cuda_runtime.h>
#include <stdio.h>

// 向量化核函数：1线程处理4个float元素
__global__ void vectorized_add_kernel(
    float* __restrict__ d_out,
    const float* __restrict__ d_in1,
    const float* __restrict__ d_in2,
    int n  // n需是4的整数倍（实际开发中需处理余数）
) {
    // 计算当前线程对应的"向量化索引"（1个索引对应4个基础元素）
    int vec_idx = threadIdx.x + blockIdx.x * blockDim.x;
    // 每个线程处理4个元素，计算基础元素的起始位置
    int base_idx = vec_idx * 4;
    
    // 边界检查：确保不越界（base_idx + 3 < n）
    if (base_idx + 3 < n) {
        // 一次性加载4个float元素（合并内存访问，效率高）
        float4 in1 = reinterpret_cast<const float4*>(d_in1)[vec_idx];
        float4 in2 = reinterpret_cast<const float4*>(d_in2)[vec_idx];
        
        // 向量化运算：4个元素同时相加（单指令处理）
        float4 out;
        out.x = in1.x + in2.x;
        out.y = in1.y + in2.y;
        out.z = in1.z + in2.z;
        out.w = in1.w + in2.w;
        
        // 一次性存储4个结果
        reinterpret_cast<float4*>(d_out)[vec_idx] = out;
    }
}

// 主机端测试函数
int main() {
    const int n = 4096;  // 必须是4的整数倍（演示用，实际需处理非4倍数场景）
    size_t size = n * sizeof(float);
    
    // 1. 主机端数据初始化
    float* h_in1 = (float*)malloc(size);
    float* h_in2 = (float*)malloc(size);
    float* h_out = (float*)malloc(size);
    for (int i = 0; i < n; i++) {
        h_in1[i] = i * 1.0f;
        h_in2[i] = 1.0f;
    }
    
    // 2. 设备端内存分配
    float *d_in1, *d_in2, *d_out;
    cudaMalloc(&d_in1, size);
    cudaMalloc(&d_in2, size);
    cudaMalloc(&d_out, size);
    
    // 3. 数据拷贝到设备
    cudaMemcpy(d_in1, h_in1, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_in2, h_in2, size, cudaMemcpyHostToDevice);
    
    // 4. 启动向量化核函数（线程数 = 总元素数 / 4）
    dim3 block(256);  // 单block线程数
    dim3 grid((n / 4 + block.x - 1) / block.x);  // 向量化后的grid大小
    vectorized_add_kernel<<<grid, block>>>(d_out, d_in1, d_in2, n);
    
    // 5. 结果拷贝回主机
    cudaMemcpy(h_out, d_out, size, cudaMemcpyDeviceToHost);
    
    // 6. 验证结果（打印前8个元素）
    for (int i = 0; i < 8; i++) {
        printf("h_out[%d] = %.1f (预期: %.1f)\n", i, h_out[i], h_in1[i] + h_in2[i]);
    }
    
    // 7. 释放内存
    free(h_in1); free(h_in2); free(h_out);
    cudaFree(d_in1); cudaFree(d_in2); cudaFree(d_out);
    
    return 0;
}
代码关键说明：
reinterpret_cast<float4*>：将普通 float 指针转为打包类型指针，实现批量内存访问；
线程数计算：向量化后线程总数 = 原始线程数 / 向量化因子（如 4），大幅减少线程调度开销；
内存合并：GPU 对连续的打包数据访问会自动合并为更少的内存事务，提升带宽利用率。
（2）进阶：针对 half 类型的向量化（__half2）
对于half（FP16）类型，CUDA 提供__half2（2 个 half 打包为 32 位），结合 Tensor Core 可实现更高效率：
cpp
运行
#include <cuda_fp16.h>

__global__ void half2_vectorized_kernel(
    half* __restrict__ d_out,
    const half* __restrict__ d_in1,
    const half* __restrict__ d_in2,
    int n
) {
    int vec_idx = threadIdx.x + blockIdx.x * blockDim.x;
    int base_idx = vec_idx * 2;
    
    if (base_idx + 1 < n) {
        // 加载2个half元素（__half2类型）
        __half2 in1 = reinterpret_cast<const __half2*>(d_in1)[vec_idx];
        __half2 in2 = reinterpret_cast<const __half2*>(d_in2)[vec_idx];
        
        // 向量化乘加运算（单指令处理2个half）
        __half2 out = hfma2(in1, in2, __float2half2_rn(1.0f));
        
        // 存储结果
        reinterpret_cast<__half2*>(d_out)[vec_idx] = out;
    }
}
（3）高级：显式 SIMD 指令（warp 级向量化）
对于 Volta 及以上架构，可使用__shfl_sync等 warp 级指令实现更精细的向量化，利用 GPU 的 SIMT warp（32 线程）特性：
cpp
运行
// warp级向量化：32线程的warp处理32*4=128个float元素
__global__ void warp_vectorized_add_kernel(
    float* __restrict__ d_out,
    const float* __restrict__ d_in1,
    const float* __restrict__ d_in2,
    int n
) {
    int warp_idx = (blockIdx.x * blockDim.x + threadIdx.x) / 32;  // warp索引
    int lane_idx = threadIdx.x % 32;  // warp内的线程索引（0-31）
    
    // 每个warp处理128个元素（32线程 × 4元素/线程）
    int base_idx = warp_idx * 128 + lane_idx * 4;
    
    if (base_idx + 3 < n) {
        float4 in1 = reinterpret_cast<const float4*>(d_in1)[base_idx/4];
        float4 in2 = reinterpret_cast<const float4*>(d_in2)[base_idx/4];
        float4 out = {in1.x+in2.x, in1.y+in2.y, in1.z+in2.z, in1.w+in2.w};
        reinterpret_cast<float4*>(d_out)[base_idx/4] = out;
    }
}
3. 向量化的关键注意事项
数据对齐：打包类型（如float4）要求内存地址按 32 字节对齐（float4=4×4=16 字节，实际 GPU 要求至少 16 字节对齐），否则会导致内存访问错误或性能下降；
解决方案：使用cudaMallocPitch分配对齐内存，或手动对齐数组大小。
余数处理：实际场景中数据量未必是向量化因子（2/4/8）的整数倍，需单独处理剩余元素：
cpp
运行
// 补充处理余数元素（非向量化部分）
int total_vec = n / 4;
int remain = n % 4;
if (vec_idx == total_vec && remain > 0) {
    int base = total_vec * 4;
    for (int i = 0; i < remain; i++) {
        d_out[base + i] = d_in1[base + i] + d_in2[base + i];
    }
}
架构适配：
Kepler/SM3.x：仅支持基础打包类型，无专用向量化指令；
Volta/SM7.x+：支持 Tensor Core 和 warp 级向量化，收益最大化；
不要过度向量化：向量化因子过大（如 8/16）可能导致单个线程计算量过高，掩盖内存延迟，需根据场景测试（常用因子：2/4）。
总结
CUDA 向量化的核心是单线程处理多个数据元素，通过打包数据类型（float2/float4/half2）实现，核心收益是提升内存带宽利用率、减少线程调度开销；
实现关键是将普通指针转为打包类型指针，调整线程数为原始数量 / 向量化因子，并处理数据对齐和余数问题；
不同 GPU 架构对向量化的支持不同，Volta 及以上架构结合 Tensor Core/warp 级指令可实现最优性能，向量化因子建议选择 2 或 4（平衡计算和内存开销）。